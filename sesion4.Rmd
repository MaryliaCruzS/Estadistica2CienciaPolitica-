---
title: "Clusters"
output: html_document
---

# An√°lisis de Clusters

<center><img src=" " width="200"/></center>

**FACULTAD DE CIENCIAS SOCIALES - PUCP**<br>

## Curso: POL 304 - Estad√≠stica para el an√°lisis pol√≠tico 2 \| Semestre 2025 - 1 <br>

## Docente: Marylia Cruz <br>

------------------------------------------------------------------------


## **¬øQu√© es un cl√∫ster?**

Un cl√∫ster es un grupo de observaciones (casos, personas, pa√≠ses, etc.) que son m√°s similares entre s√≠ que con las observaciones de otros grupos.

En otras palabras, el an√°lisis de cl√∫ster busca agrupar los datos de forma que los elementos dentro de cada grupo sean muy parecidos, y entre grupos sean distintos.

Este tipo de an√°lisis es no supervisado, es decir, no parte de etiquetas conocidas (como categor√≠as o clases), sino que descubre patrones ocultos en los datos.

## **¬øQu√© es la matriz de distancia?**

Para saber qu√© tan parecidas o diferentes son dos observaciones, usamos una medida de distancia.

La m√°s com√∫n es la distancia euclidiana, que es simplemente la distancia ‚Äúen l√≠nea recta‚Äù entre dos puntos en un espacio con m√∫ltiples dimensiones.

## üìê F√≥rmula de la distancia euclidiana

La distancia euclidiana entre dos observaciones \( A \) y \( B \), con \( n \) variables, se define como:

\[
d(A, B) = \sqrt{(x_1^A - x_1^B)^2 + (x_2^A - x_2^B)^2 + \dots + (x_n^A - x_n^B)^2}
\]

Esta f√≥rmula calcula la distancia en l√≠nea recta entre los puntos \( A \) y \( B \) en un espacio de \( n \) dimensiones.

## Ejemplo

```{r}
datos <- data.frame(
  Pais = c("Per√∫", "Chile", "Noruega", "Argentina"),
  Educacion = c(80, 85, 95, 82),
  ConfianzaGobierno = c(30, 35, 85, 28)
)

# Usamos el nombre del pa√≠s como nombre de fila
rownames(datos) <- datos$Pais
datos_numericos <- datos[, -1]  # Quitamos la columna "Pais"

```

```{r}
distancias <- dist(datos_numericos, method = "euclidean")
as.matrix(distancias)
distancias
```


## **Objetivo**

Aplicar t√©cnicas de an√°lisis de cl√∫ster (jer√°rquico y k-medias) a un conjunto de datos para identificar agrupaciones de observaciones con caracter√≠sticas similares.


El an√°lisis de cl√∫ster es una t√©cnica de clasificaci√≥n no supervisada que permite agrupar observaciones con base en su similitud. Entre los m√©todos m√°s comunes est√°n:

# **Cl√∫ster Jer√°rquico**

*Cl√∫ster Jer√°rquico:* agrupa observaciones de forma aglomerativa o divisiva, creando un dendrograma.

- Este m√©todo agrupa las observaciones progresivamente en una estructura en forma de √°rbol, conocida como dendrograma. Existen dos enfoques:

Aglomerativo (el m√°s usado): parte de cada observaci√≥n como su propio grupo y las va uniendo.

Divisivo: parte de un solo grupo y lo va dividiendo.

1. Calcular la matriz de distancias entre todas las observaciones.

2. Unir los grupos m√°s cercanos (seg√∫n un criterio de enlace: Ward, promedio, completo, etc.).

3. Repetir hasta que todos est√©n en un solo cl√∫ster.

4. Elegir un n√∫mero de cl√∫steres cortando el dendrograma a cierta altura.

*Ventajas*

- No requiere definir previamente el n√∫mero de cl√∫steres.

- Permite ver c√≥mo se forman los grupos a lo largo del proceso.

- √ötil para datos peque√±os o exploratorios

*Desventajas*

- Poco eficiente para grandes vol√∫menes de datos.

- Decisiones iniciales (como el m√©todo de enlace) afectan el resultado.


1. Cargar datos

```{r}
library(rio)
data=import("data_wvs_consolidada.xlsx")
```


```{r}
rownames(data) <- data$Country
```

2. Preparar datos num√©ricos y escalar

```{r}
library(dplyr)
# Escalamos los datos para evitar sesgos por diferencias de magnitud

data_numerica <- data %>% 
  dplyr::select(-Country) %>% 
  dplyr::select_if(is.numeric)


data_escalada=scale(data_numerica)
data_escalada=as.data.frame(data_escalada)
```


3. Calcular matriz de distancias



Calculamos las distancias euclidianas entre todas las observaciones. Esta matriz ser√° la base para formar los grupos.


```{r}
distancias <- dist(data_escalada, method = "euclidean")
```


4. Aplicar cl√∫ster jer√°rquico (m√©todo Ward)

Utilizamos el m√©todo aglomerativo de Ward (ward.D2), que minimiza la varianza dentro de cada grupo al ir fusionando observaciones.

```{r}
hc <- hclust(distancias, method = "ward.D2")
```


5. Visualizar dendrograma

El dendrograma muestra gr√°ficamente c√≥mo se agrupan las observaciones paso a paso. Podemos elegir el n√∫mero de cl√∫steres ‚Äúcortando‚Äù el √°rbol a cierta altura. Aqu√≠ sugerimos usar 4 grupos.

```{r}
plot(hc, hang = -1, main = "Dendrograma de Cl√∫ster Jer√°rquico")
```


- A√±adimos colores para visualizar 4 cl√∫steres

```{r}
#rect.hclust(hc, k = 4, border = 2:5)
```

*6. Cortar en 4 cl√∫steres*

Asignamos cada observaci√≥n a uno de los 4 grupos, seg√∫n el corte que hicimos en el dendrograma.

```{r}
grupos <- cutree(hc, k = 4)
```



- Revisamos cu√°ntas observaciones hay en cada grupo
```{r}
table(grupos) 
```


7. Agregar cl√∫ster al dataframe original

Agregamos la asignaci√≥n de cl√∫ster como una nueva variable en el conjunto de datos original. Esto permite luego comparar perfiles de grupo.

```{r}
data_escalada$Cluster <- as.factor(grupos)
```

8. Promedios por cl√∫ster

Calculamos los promedios de cada variable num√©rica por cl√∫ster para caracterizar a cada grupo y entender qu√© los diferencia.

```{r}
library(dplyr)
data_escalada%>%
  group_by(Cluster) %>%
  summarise(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))
```


# **Cl√∫ster K-medias**
  

*Cl√∫ster K-medias:* particiona los datos en k grupos al minimizar la varianza dentro de cada cl√∫ster.

Es un m√©todo particional: divide directamente los datos en k grupos. Funciona minimizando la variabilidad interna dentro de cada cl√∫ster (inercia).

- Definir el n√∫mero de cl√∫steres k.

- Inicializar k centros aleatorios.

- Asignar cada observaci√≥n al centro m√°s cercano.

- Recalcular los centros como el promedio de las observaciones en cada grupo.

- Repetir pasos 3‚Äì4 hasta que no cambien los grupos

*Ventajas* 

- Muy eficiente en grandes bases de datos.

- F√°cil de interpretar y aplicar.

**Desventajas**

- Requiere definir k antes de empezar.

- Sensible a valores at√≠picos y a la escala de los datos.

- Puede converger a soluciones sub√≥ptimas si los centros se inicializan mal (por eso se usa nstart = 25, por ejemplo, para mejorar la soluci√≥n)



**1. Determinar el n√∫mero √≥ptimo de cl√∫steres**

Antes de aplicar k-medias, se recomienda determinar el n√∫mero √≥ptimo de cl√∫steres. Una t√©cnica muy usada es el m√©todo del codo (elbow method), que eval√∫a la variaci√≥n dentro de los grupos (WSS).

```{r}
data_escalada <- na.omit(data_escalada)
```


```{r}
library(factoextra)
fviz_nbclust(data_escalada, kmeans, method = "wss") +
  labs(title = "M√©todo del codo para determinar K √≥ptimo")
```


  
- Observa en el gr√°fico el punto donde la curva ‚Äúdeja de bajar bruscamente‚Äù ‚Äî ah√≠ estar√≠a el n√∫mero ideal de cl√∫steres.

*3. Aplicar el algoritmo de k-medias (k = 4)*

Usamos kmeans() con k = 4 como ejemplo (puedes ajustar seg√∫n el gr√°fico anterior). Tambi√©n se recomienda usar nstart = 25 para mejorar la estabilidad del resultado (intenta 25 inicializaciones distintas).

```{r}
set.seed(123)  # para reproducibilidad
kmeans_result <- kmeans(data_escalada, centers = 4, nstart = 25)
```


*Ver n√∫mero de casos por grupo*

```{r}
kmeans_result$size
```

*4. Asignar cl√∫steres al dataframe original*

Agregamos la asignaci√≥n del cl√∫ster como una nueva columna en el dataframe original. Esto nos permitir√° analizar las diferencias entre grupos.

```{r}
data_escalada$Cluster_kmeans <- as.factor(kmeans_result$cluster)
```

*5. Visualizaci√≥n de los cl√∫steres*

Graficamos los cl√∫steres usando reducci√≥n de dimensiones (PCA) para representar los datos en 2D y colorearlos por cl√∫ster.

```{r}
library(factoextra)
#fviz_cluster(kmeans_result, data = data_escalada,
            #palette = "jco",
             #ggtheme = theme_minimal(),
             #main = "Cl√∫steres K-means")

```


*6. Comparar promedios por cl√∫ster*

Este paso nos ayuda a interpretar cada grupo: qu√© valores promedio tiene cada cl√∫ster en las variables num√©ricas.

```{r}
data_escalada %>%
  group_by(Cluster_kmeans) %>%
  summarise(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))
```
```{r}
# Crear tabla con pa√≠ses y su cl√∫ster asignado
tabla_cluster_paises <- data.frame(
  Pais = rownames(data_escalada),
  Cluster = as.factor(kmeans_result$cluster)
)

# Ver tabla ordenada por cl√∫ster
tabla_cluster_paises <- tabla_cluster_paises %>% arrange(Cluster)

# Mostrar la tabla
print(tabla_cluster_paises)

```


<br></br>

</style>

<a href="">
  <button>Descargar la base de datos</button>
</a>

<br></br> [Go to page beginning](#beginning)

